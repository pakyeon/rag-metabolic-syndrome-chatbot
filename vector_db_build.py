import os
import re
import glob
import logging
from typing import List, Dict, Any, Optional

import random
import numpy as np
import torch


def set_global_seed(seed: int = 1) -> None:
    """ì‹œë“œ ê³ ì •: ì¬í˜„ì„± í™•ë³´"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


set_global_seed(1)

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,
    MarkdownHeaderTextSplitter,
)
from langchain_chroma import Chroma
from langchain_core.documents import Document
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
)
logger = logging.getLogger(__name__)


class VectorDBBuilder:
    def __init__(
        self,
        embedding_model_name: str = "Qwen/Qwen3-Embedding-0.6B",
        chromadb_path: str = "./chromadb",
        # ğŸ”§ ì²­í¬ ë¶„ì„/ì„ê³„ê°’ íŒŒë¼ë¯¸í„°í™”
        min_content_length: int = 20,
        min_chunk_size: int = 50,
        max_merge_size: int = 800,
    ) -> None:
        """
        ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œìŠ¤í…œ ì´ˆê¸°í™”

        Args:
            embedding_model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…
            chromadb_path: ChromaDB ì €ì¥ ê²½ë¡œ
            min_content_length: í—¤ë”ë§Œ ìˆëŠ” ì²­í¬ íŒì • ì‹œ ë‚´ìš© ìµœì†Œ ê¸¸ì´
            min_chunk_size: ë³‘í•© ëŒ€ìƒ ìµœì†Œ ì²­í¬ í¬ê¸°
            max_merge_size: ë³‘í•© ê²°ê³¼ ìƒí•œ í¬ê¸°
        """
        self.embedding_model_name = embedding_model_name
        self.chromadb_path = chromadb_path
        self.embeddings = None
        self.db: Optional[Chroma] = None

        # thresholds
        self.min_content_length = min_content_length
        self.min_chunk_size = min_chunk_size
        self.max_merge_size = max_merge_size

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_embeddings()

    def _initialize_embeddings(self) -> None:
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # [0-2] ì •ê·œí™” í™œì„±í™”: index ì¸¡ê³¼ query ì¸¡ì„ ë°˜ë“œì‹œ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•¨ :contentReference[oaicite:12]{index=12}
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model_name,
                encode_kwargs={"normalize_embeddings": True},
            )
            logger.info(
                "ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: %s (normalize_embeddings=True)",
                self.embedding_model_name,
            )
        except Exception as e:  # pragma: no cover
            logger.error("ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: %s", str(e))
            raise

    def load_md_files_from_folder(self, folder_path: str) -> List[Document]:
        """
        í´ë”ì—ì„œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        """
        documents: List[Document] = []

        if not os.path.exists(folder_path):
            logger.warning("í´ë” '%s'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", folder_path)
            return documents

        md_files = glob.glob(os.path.join(folder_path, "*.md"))
        if not md_files:
            logger.warning("í´ë” '%s'ì— .md íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", folder_path)
            return documents

        logger.info("ë°œê²¬ëœ .md íŒŒì¼ ìˆ˜: %d", len(md_files))

        for file_path in md_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    documents.append(
                        Document(page_content=content, metadata={"source": file_path})
                    )
            except Exception as e:  # pragma: no cover
                logger.error("íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ (%s): %s", file_path, str(e))

        return documents

    def _is_header_only_chunk(self, content: str) -> bool:
        """
        ì²­í¬ê°€ í—¤ë”ë§Œ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í™•ì¸
        """
        header_pattern = r"^#{1,6}\s+.+$"
        lines = content.strip().split("\n")
        non_empty_lines = [line.strip() for line in lines if line.strip()]
        if not non_empty_lines:
            return True

        header_only = all(re.match(header_pattern, line) for line in non_empty_lines)

        content_without_headers = "\n".join(
            [line for line in non_empty_lines if not re.match(header_pattern, line)]
        )
        too_short = len(content_without_headers.strip()) < self.min_content_length
        return header_only or too_short

    def _get_header_level(self, metadata: Dict[str, Any]) -> int:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ìµœìƒìœ„ í—¤ë” ë ˆë²¨ì„ ìœ ì¶”"""
        header_keys = sorted([k for k in metadata.keys() if k.startswith("Header")])
        if not header_keys:
            return 0
        levels = []
        for k in header_keys:
            try:
                lvl = int(re.findall(r"\d+", k)[0])
                levels.append(lvl)
            except Exception:
                continue
        return min(levels) if levels else 0

    def _can_merge_chunks(
        self, chunk1_metadata: Dict[str, Any], chunk2_metadata: Dict[str, Any]
    ) -> bool:
        """
        ë‘ ì²­í¬ê°€ í—¤ë” ê³„ì¸µ êµ¬ì¡°ìƒ ë³‘í•© ê°€ëŠ¥í•œì§€ í™•ì¸
        (ë¹„ì¦ˆë‹ˆìŠ¤ ë£°ì€ ë³´ìˆ˜ì ìœ¼ë¡œ ìœ ì§€)
        """
        level1 = self._get_header_level(chunk1_metadata)
        level2 = self._get_header_level(chunk2_metadata)

        # ë‘˜ ë‹¤ í—¤ë”ê°€ ì—†ìœ¼ë©´ ë³‘í•© ê°€ëŠ¥
        if level1 == 0 and level2 == 0:
            return True

        # ê°™ì€ ë ˆë²¨ì´ë©´ ë³‘í•© ê°€ëŠ¥
        if level1 == level2 and level1 > 0:
            return True

        # ì²« ë²ˆì§¸ê°€ ìƒìœ„ í—¤ë”ì´ê³  ë‘ ë²ˆì§¸ê°€ ê·¸ë³´ë‹¤ í•˜ìœ„ ë ˆë²¨ì´ë©´ (ì˜ˆ: 2 -> 3, 2 -> 4) ë‘ ë‹¨ê³„ê¹Œì§€ í—ˆìš©
        if level1 > 0 and (level1 < level2 <= level1 + 2):
            return True

        return False

    def _dedup_overlap(self, left: str, right: str, max_check: int = 1000) -> str:
        """
        left(=í˜„ì¬ê¹Œì§€ ë³‘í•©ëœ í…ìŠ¤íŠ¸)ì˜ 'ì ‘ë¯¸ì‚¬'ì™€
        right(=ë‹¤ìŒ ì²­í¬)ì˜ 'ì ‘ë‘ì‚¬'ê°€ ê²¹ì¹˜ë©´ ê·¸ ê²¹ì¹˜ëŠ” ê¸¸ì´ë§Œí¼ rightë¥¼ ì˜ë¼ ë°˜í™˜.
        - right ì „ì²´ê°€ ì´ë¯¸ left ì•ˆì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜(ì¤‘ë³µ ì‚½ì… ë°©ì§€).
        - max_check: ê²¹ì¹¨ íƒìƒ‰ ìµœëŒ€ ê¸¸ì´(ì„±ëŠ¥/ì•ˆì •ì„±ìš©)
        """
        if not left or not right:
            return right

        # ì™„ì „ í¬í•¨: right ì „ì²´ê°€ ì´ë¯¸ í¬í•¨ëœ ê²½ìš°
        if right.strip() and right in left:
            return ""

        max_len = min(len(left), len(right), max_check)

        # ê¸´ ê²¹ì¹¨ë¶€í„° íƒìƒ‰
        for k in range(max_len, 0, -1):
            if left[-k:] == right[:k]:
                return right[k:]

        return right

    def _merge_small_chunks(self, chunks: List[Document]) -> List[Document]:
        """
        ì‘ì€ ì²­í¬/í—¤ë”ë§Œ ìˆëŠ” ì²­í¬ë¥¼ ë‹¤ìŒ ì²­í¬ì™€ ë³‘í•© (ë¬¸ì„œ ê²½ê³„ ë³´ì¡´)
        """
        if not chunks:
            return chunks

        merged_chunks: List[Document] = []
        i = 0
        while i < len(chunks):
            current_chunk = chunks[i]
            current_content = current_chunk.page_content.strip()
            current_source = current_chunk.metadata.get("source")

            # ì¶©ë¶„íˆ í¬ê³  ì˜ë¯¸ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì±„íƒ
            if len(
                current_content
            ) >= self.min_chunk_size and not self._is_header_only_chunk(
                current_content
            ):
                merged_chunks.append(current_chunk)
                i += 1
                continue

            # ë³‘í•© ì‹œì‘
            merged_content = current_content
            merged_metadata = dict(current_chunk.metadata)
            j = i + 1

            while j < len(chunks) and len(merged_content) < self.max_merge_size:
                next_chunk = chunks[j]

                # ë‹¤ë¥¸ íŒŒì¼ì´ë©´ ë³‘í•© ì¤‘ë‹¨ (ë¬¸ì„œ ê²½ê³„ ë³´í˜¸)
                if next_chunk.metadata.get("source") != current_source:
                    break

                if not self._can_merge_chunks(merged_metadata, next_chunk.metadata):
                    break

                next_content = next_chunk.page_content.strip()

                # 1) ê²¹ì¹˜ëŠ” ì ‘ë¯¸ì‚¬/ì ‘ë‘ì‚¬ ì œê±°
                deduped_next = self._dedup_overlap(
                    merged_content,
                    next_content,
                    max_check=self.max_merge_size,
                )

                # 2) ì™„ì „ í¬í•¨ì´ë©´(ì¶”ê°€í•  ë‚´ìš© ì—†ìŒ) ë‹¤ìŒ ì²­í¬ë¡œ ì§„í–‰
                if deduped_next == "":
                    j += 1
                    continue

                # 3) ì‹¤ì œ ì´ì–´ë¶™ì˜€ì„ ë•Œ ê¸¸ì´ ê²€ì‚¬
                candidate = (
                    merged_content
                    + ("\n\n" if merged_content and deduped_next else "")
                    + deduped_next
                )
                if len(candidate) > self.max_merge_size:
                    break

                merged_content = candidate

                # í—¤ë” ë©”íƒ€ë°ì´í„° ë³´ìˆ˜ì  ë³‘í•©
                for k, v in next_chunk.metadata.items():
                    if k not in merged_metadata:
                        merged_metadata[k] = v

                # ëª©í‘œ í¬ê¸° ë„ë‹¬ ì‹œ ì¢…ë£Œ
                if len(
                    merged_content
                ) >= self.min_chunk_size and not self._is_header_only_chunk(
                    merged_content
                ):
                    j += 1
                    break

                j += 1

            if merged_content.strip():
                merged_chunks.append(
                    Document(page_content=merged_content, metadata=merged_metadata)
                )

            i = j

        return merged_chunks

    @staticmethod
    def _compose_header_path(md: dict) -> str:
        """[0-1] Header 1~3ì„ ' / 'ë¡œ ì—°ê²°í•œ ê²½ë¡œ ë¬¸ìì—´ ìƒì„± (ì •ì  ë©”ì„œë“œë¡œ ìˆ˜ì •)"""
        parts = [md.get("Header 1"), md.get("Header 2"), md.get("Header 3")]
        parts = [p.strip() for p in parts if isinstance(p, str) and p.strip()]
        return " / ".join(parts)

    def split_documents_with_markdown_headers(
        self,
        documents: List[Document],
        chunk_size: int = 400,
        chunk_overlap: int = 40,
    ) -> List[Document]:
        # âœ… í—¤ë” ë ˆë²¨ 1~3ê¹Œì§€ë§Œ ì‚¬ìš©
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
            ],
            strip_headers=False,
        )
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )

        all_chunks: List[Document] = []
        for doc in documents:
            md_header_splits = markdown_splitter.split_text(doc.page_content)

            for split_doc in md_header_splits:
                # ì›ë³¸ + í—¤ë” ë©”íƒ€ë°ì´í„° ë³‘í•©
                merged_md = {**doc.metadata, **split_doc.metadata}

                # âœ… [0-1] header_pathë¥¼ ì¸ë±ì‹± ì‹œì ì— ìƒì„±Â·ì €ì¥
                header_path = self._compose_header_path(merged_md)
                merged_md["header_path"] = header_path

                if len(split_doc.page_content) > chunk_size:
                    for chunk in text_splitter.split_text(split_doc.page_content):
                        all_chunks.append(
                            Document(page_content=chunk, metadata=merged_md)
                        )
                else:
                    all_chunks.append(
                        Document(
                            page_content=split_doc.page_content, metadata=merged_md
                        )
                    )

        # (ì„ íƒ) ì‘ì€ ì²­í¬ ë³‘í•© ë¡œì§ ìœ ì§€
        merged_chunks = self._merge_small_chunks(all_chunks)
        return merged_chunks

    def remove_duplicates(self, chunks: List[Document]) -> List[Document]:
        """
        ì¤‘ë³µë˜ëŠ” ì¡°ê°ë“¤ì„ ì œê±° (ìˆœì„œëŠ” ìœ ì§€)
        - ì¤‘ë³µ íŒë‹¨ í‚¤: (ì •ê·œí™”ëœ ë‚´ìš©, source)
        """
        logger.info("ì¤‘ë³µ ì œê±° ì¤‘...")
        unique_chunks: List[Document] = []
        seen: set = set()

        def _normalize_text(t: str) -> str:
            return re.sub(r"\s+", " ", t.strip())

        for doc in chunks:
            norm = _normalize_text(doc.page_content)
            key = (norm, doc.metadata.get("source"))
            if key not in seen:
                seen.add(key)
                unique_chunks.append(doc)

        logger.info("ì¤‘ë³µ ì œê±° í›„ ì¡°ê° ìˆ˜: %d", len(unique_chunks))
        return unique_chunks

    def create_vector_database(self, chunks: List[Document]) -> Chroma:
        """
        ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± (ê¸°ì¡´ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚­ì œ í›„ ìƒì„±)
        """
        if os.path.exists(self.chromadb_path):
            import shutil

            shutil.rmtree(self.chromadb_path)
            logger.info("ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ")

        self.db = Chroma.from_documents(
            chunks,
            self.embeddings,
            persist_directory=self.chromadb_path,
        )
        try:
            # ì¼ë¶€ ë²„ì „ì—ì„œ ëª…ì‹œ persistê°€ ì•ˆì „
            self.db.persist()  # type: ignore[attr-defined]
        except Exception:
            pass

        logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: %s", self.chromadb_path)
        return self.db

    def analyze_chunks(self, chunks: List[Document]) -> Dict[str, Any]:
        """ì²­í¬ í†µê³„ ì •ë³´ ì œê³µ"""
        if not chunks:
            return {"ì´ ì²­í¬ ìˆ˜": 0}

        lengths = [len(doc.page_content) for doc in chunks]
        return {
            "ì´ ì²­í¬ ìˆ˜": len(chunks),
            "ìµœì†Œ ê¸¸ì´": min(lengths),
            "ìµœëŒ€ ê¸¸ì´": max(lengths),
            "í‰ê·  ê¸¸ì´": sum(lengths) // len(lengths),
        }

    def build_from_folder(
        self, folder_path: str, chunk_size: int = 400, chunk_overlap: int = 40
    ) -> bool:
        """
        í´ë”ë¡œë¶€í„° ì „ì²´ íŒŒì´í”„ë¼ì¸ ìˆ˜í–‰
        """
        try:
            documents = self.load_md_files_from_folder(folder_path)
            if not documents:
                logger.error("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False

            chunks = self.split_documents_with_markdown_headers(
                documents, chunk_size, chunk_overlap
            )

            analysis = self.analyze_chunks(chunks)
            logger.info("\n=== ì²­í¬ ë¶„ì„ ê²°ê³¼ ===")
            for k, v in analysis.items():
                logger.info("%s: %s", k, v)

            unique_chunks = self.remove_duplicates(chunks)
            logger.info("ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ì²­í¬ ìˆ˜: %d", len(unique_chunks))

            self.create_vector_database(unique_chunks)
            return True
        except Exception as e:  # pragma: no cover
            logger.exception("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜: %s", str(e))
            return False

    def get_database_info(self) -> Dict[str, Any]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì œê³µ
        """
        try:
            if not self.db:
                return {"status": "ë°ì´í„°ë² ì´ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•ŠìŒ"}

            try:
                got = self.db.get(include=[])  # idsë§Œ
                count = len(got.get("ids", []))
            except Exception:
                count = None

            info = {
                "status": "êµ¬ì¶• ì™„ë£Œ",
                "total_documents": count,
                "embedding_model": self.embedding_model_name,
                "storage_path": self.chromadb_path,
                "database_size": self._get_directory_size(self.chromadb_path),
            }
            return info
        except Exception as e:  # pragma: no cover
            return {"status": f"ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}"}

    def _get_directory_size(self, path: str) -> str:
        """ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°"""
        total_size = 0
        for dirpath, _, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(filepath)
                except Exception:
                    pass
        return f"{total_size / (1024*1024):.2f} MB"


def main() -> None:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì„¤ì •
    documents_folder = "./data"  # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”
    embedding_model = "nlpai-lab/KURE-v1"  # ì„ë² ë”© ëª¨ë¸
    chromadb_path = f"./chromadb/v2/{embedding_model}"  # ë²¡í„°DB ì €ì¥ ê²½ë¡œ
    chunk_size = 500  # í…ìŠ¤íŠ¸ ì¡°ê° í¬ê¸°
    chunk_overlap = 50  # ì¡°ê° ê°„ ê²¹ì¹¨

    load_dotenv()

    builder = VectorDBBuilder(
        embedding_model_name=embedding_model,
        chromadb_path=chromadb_path,
        # ğŸ”§ í•„ìš” ì‹œ ì„ê³„ê°’ ì¡°ì • ê°€ëŠ¥
        min_content_length=30,
        min_chunk_size=100,
        max_merge_size=1000,
    )

    success = builder.build_from_folder(
        folder_path=documents_folder,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )

    if success:
        db_info = builder.get_database_info()
        logger.info("\n=== ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ===")
        for key, value in db_info.items():
            logger.info("%s: %s", key, value)

        logger.info("\në²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("ì´ì œ retriever_evaluate.pyë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        logger.error("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
