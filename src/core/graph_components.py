from __future__ import annotations
from typing import List, TypedDict, Optional, Dict, Any

import torch
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from src import config
from src.core.reranker import reranker

# --- LLM Clients ---
llm = ChatOpenAI(model=config.LLM_MODEL, temperature=config.LLM_TEMPERATURE)
detect_llm = ChatOpenAI(model=config.DETECT_LLM_MODEL, temperature=0)


# --- State ---
class RAGState(TypedDict, total=False):
    question: str
    is_related: bool
    raw_docs: List[Document]
    scores: List[float]
    prepared: List[dict]
    reranked: List[dict]
    context: str
    answer: str
    error: Optional[str]
    # ë©”ëª¨ë¦¬ ê´€ë ¨ í•„ë“œ ì¶”ê°€
    conversation_history: Optional[str]
    user_id: Optional[str]
    conversation_id: Optional[str]


# --- Nodes ---
def n_classify(state: RAGState) -> RAGState:
    """ì§ˆë¬¸ì´ ì£¼ì œì™€ ê´€ë ¨ ìˆëŠ”ì§€ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    q = state["question"]
    prompt = ChatPromptTemplate.from_template(
        "ì§ˆë¬¸ì´ 'ëŒ€ì‚¬ì¦í›„êµ°'ê³¼ ê´€ë ¨ ìˆëŠ”ì§€ 'yes' ë˜ëŠ” 'no'ë¡œë§Œ ë‹µí•˜ì‹œì˜¤.\nì§ˆë¬¸: {q}"
    )
    resp = (prompt | detect_llm).invoke({"q": q})
    related = resp.content.strip().lower().startswith("y")
    return {"is_related": related}


def n_retrieve(state: RAGState, db) -> RAGState:
    """DBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    q = state["question"]
    try:
        results_with_scores = db.similarity_search_with_relevance_scores(
            q, k=config.TOP_K
        )
        if results_with_scores:
            docs, scores = zip(*results_with_scores)
            return {"raw_docs": list(docs), "scores": [float(s) for s in scores]}
        return {"raw_docs": [], "scores": []}
    except Exception:
        # relevance scoreë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¼ë¶€ ChromaDB ë²„ì „ì„ ìœ„í•œ í´ë°±
        docs = db.similarity_search(q, k=config.TOP_K)
        return {"raw_docs": docs, "scores": []}


def n_prepare_docs(state: RAGState) -> RAGState:
    """
    [ìˆ˜ì •ë¨] ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í‘œì¤€ í˜•íƒœë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
    (basenameì„ ì¶”ê°€ë¡œ ì¶”ì¶œ)
    """
    docs = state.get("raw_docs", [])
    scores = state.get("scores", [])
    if not docs:
        return {"prepared": []}

    prepared_docs = []
    for d, s in zip(docs, scores) if scores else [(d, None) for d in docs]:
        md = getattr(d, "metadata", {}) or {}
        prepared_docs.append(
            {
                "content": d.page_content,
                "basename": md.get("basename", "ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ"),
                "header_path": md.get("header_path", ""),
                "retrieval_score": float(s) if s is not None else None,
            }
        )
    return {"prepared": prepared_docs}


def _format_rerank_instruction(query: str, doc: str) -> str:
    """Rerankerë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì„ ë§Œë“­ë‹ˆë‹¤."""
    instruction = "Given a query, judge whether the Document helps answer the query."
    return f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"


def _rerank_docs(query: str, doc_list: List[dict], top_k: int = 5) -> List[dict]:
    """ë¬¸ì„œ ëª©ë¡ì„ ë¦¬ë­í‚¹í•©ë‹ˆë‹¤."""
    if not reranker.ensure_loaded():
        print("[WARN] Reranker ë¡œë“œ ì‹¤íŒ¨, ê²€ìƒ‰ ìˆœì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return doc_list[:top_k]

    try:
        pairs = [_format_rerank_instruction(query, d["content"]) for d in doc_list]

        # í† í°í™” ë° ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
        inputs = reranker.tokenizer(
            pairs,
            padding=False,
            truncation="longest_first",
            max_length=reranker.max_len
            - len(reranker.prefix_tokens)
            - len(reranker.suffix_tokens),
            return_attention_mask=False,
        )
        for i in range(len(inputs["input_ids"])):
            inputs["input_ids"][i] = (
                reranker.prefix_tokens + inputs["input_ids"][i] + reranker.suffix_tokens
            )

        padded_inputs = reranker.tokenizer.pad(
            inputs, padding=True, return_tensors="pt"
        ).to(reranker.model.device)

        # ì ìˆ˜ ê³„ì‚°
        with torch.no_grad():
            logits = reranker.model(**padded_inputs).logits[:, -1, :]
            scores = torch.softmax(logits, dim=-1)[:, reranker.token_true]

        # ì ìˆ˜ ì¶”ê°€ ë° ì •ë ¬
        for doc, score in zip(doc_list, scores.tolist()):
            doc["rerank_score"] = float(score)

        reranked_list = sorted(doc_list, key=lambda x: x["rerank_score"], reverse=True)
        return reranked_list[:top_k]

    except Exception as e:
        print(f"[WARN] ë¦¬ë­í‚¹ ì‹¤íŒ¨, ê²€ìƒ‰ ìˆœì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ìœ : {e}")
        return doc_list[:top_k]


def n_rerank(state: RAGState) -> RAGState:
    """í•„ìš” ì‹œ ë¬¸ì„œë¥¼ ë¦¬ë­í‚¹í•©ë‹ˆë‹¤."""
    scored = state.get("scored", [])
    if not scored:
        return {"reranked": []}
    if not config.RAG_USE_RERANK:
        return {"reranked": scored[:5]}

    reranked_docs = _rerank_docs(state["question"], scored, top_k=min(5, len(scored)))
    return {"reranked": reranked_docs}


def n_build_context(state: RAGState) -> RAGState:
    """LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    items = state.get("reranked") or state.get("prepared") or []
    lines = []
    for i, d in enumerate(items, 1):
        basename = d.get("basename", "ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ")
        header_path = d.get("header_path") or "ì „ì²´ ë‚´ìš©"
        lines.append(
            f"{i}.\n"
            f"ì¶œì²˜: {basename}\n"
            f"ê²½ë¡œ: {header_path}\n"
            f"ë‚´ìš©: {d.get('content','')}\n"
        )
    return {"context": "\n\n".join(lines)}


def n_generate_rag(state: RAGState) -> RAGState:
    """ê°œì„ ëœ RAG ê¸°ë°˜ ë‹µë³€ ìƒì„± í•¨ìˆ˜"""

    conversation_history = state.get("conversation_history", "")

    # ê³µí†µ ì‹œìŠ¤í…œ ë©”ì‹œì§€ (êµ¬ì¡°í™” ë° ì˜ë£Œ ë©´ì±…ì‚¬í•­ ì¶”ê°€)
    system_message = """ë‹¹ì‹ ì€ ëŒ€ì‚¬ì¦í›„êµ° ìƒë‹´ì‚¬ë¥¼ ì§€ì›í•˜ëŠ” ì „ë¬¸ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

## ì—­í•  ë° ì±…ì„
- ìƒë‹´ì‚¬ê°€ í™˜ìì—ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì •ë³´ ì œê³µ
- ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ ì‘ì„±
- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ë©°, ì „ë¬¸ì ì´ë©´ì„œë„ ì¹œê·¼í•œ í†¤ ìœ ì§€

## ë‹µë³€ í˜•ì‹ ê°€ì´ë“œë¼ì¸
1. **êµ¬ì¡°í™”ëœ ë‹µë³€**: ì£¼ì œë³„ë¡œ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì‘ì„±
2. **ê°€ë…ì„± ìµœì í™”**: ë¬¸ë‹¨, ì¤„ë°”ê¿ˆ, ë²ˆí˜¸/ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ì ê·¹ í™œìš©
3. **í•µì‹¬ ì •ë³´ ê°•ì¡°**: ì¤‘ìš”í•œ ë‚´ìš©ì€ **êµµê²Œ** í‘œì‹œ
4. **ë‹¨ê³„ë³„ ì„¤ëª…**: ë³µì¡í•œ ë‚´ìš©ì€ ìˆœì„œëŒ€ë¡œ ë‹¨ê³„ë³„ ì„¤ëª…
5. **ì‹¤ìš©ì  ì¡°ì–¸**: í™˜ìê°€ ì‹¤ì œë¡œ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì  ë°©ë²• ì œì‹œ

## ì˜ë£Œ ì •ë³´ ê´€ë ¨ ì£¼ì˜ì‚¬í•­
âš ï¸ **ì¤‘ìš”**: ì œê³µë˜ëŠ” ì •ë³´ëŠ” ì¼ë°˜ì ì¸ êµìœ¡ ëª©ì ì´ë©°, ê°œë³„ í™˜ìì˜ êµ¬ì²´ì ì¸ ì˜ë£Œ ìƒë‹´ì´ë‚˜ ì§„ë‹¨ì„ ëŒ€ì²´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™˜ìë³„ ë§ì¶¤ ì¹˜ë£Œë‚˜ ì•½ë¬¼ ì¡°ì ˆì€ ë°˜ë“œì‹œ ë‹´ë‹¹ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

## ì¶œì²˜ í‘œê¸° ê·œì¹™
ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì°¸ê³  ìë£Œë¥¼ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤:

ğŸ“š **ì°¸ê³  ìë£Œ**
- ì¶œì²˜: '[ë¬¸ì„œëª…]'
  ê²½ë¡œ: '[ìƒì„¸ ê²½ë¡œ]'
"""

    if conversation_history:
        user_message = """## ì´ì „ ëŒ€í™” ë‚´ìš©
{conversation_history}

## í˜„ì¬ ì§ˆë¬¸
{question}

## ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ
{documents}

ìœ„ì˜ ì´ì „ ëŒ€í™” ë§¥ë½ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ëª¨ë‘ ì°¸ê³ í•˜ì—¬, í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì²´ê³„ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."""
    else:
        user_message = """## ì§ˆë¬¸
{question}

## ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ
{documents}

ìœ„ì˜ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì²´ê³„ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_message),
            ("user", user_message),
        ]
    )

    chain = prompt | llm
    invoke_params = {
        "question": state["question"],
        "documents": state.get("context", ""),
    }

    if conversation_history:
        invoke_params["conversation_history"] = conversation_history

    resp = chain.invoke(invoke_params)
    return {"answer": resp.content}


def n_generate_direct(state: RAGState) -> RAGState:
    """ê°œì„ ëœ ì§ì ‘ ë‹µë³€ ìƒì„± í•¨ìˆ˜"""

    conversation_history = state.get("conversation_history", "")

    # ê³µí†µ ì‹œìŠ¤í…œ ë©”ì‹œì§€
    system_message = """ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ë‹µë³€ ì›ì¹™
- í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€
- êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì •ë³´ ì œê³µ
- ì „ë¬¸ì ì´ë©´ì„œë„ ì¹œê·¼í•œ í†¤ ìœ ì§€

## ë‹µë³€ í˜•ì‹ ê°€ì´ë“œë¼ì¸
1. **ëª…í™•í•œ êµ¬ì¡°**: ì£¼ì œë³„ë¡œ ì²´ê³„ì  êµ¬ì„±
2. **ê°€ë…ì„±**: ë¬¸ë‹¨, ëª©ë¡, ê°•ì¡° í‘œì‹œ í™œìš©
3. **ì‹¤ìš©ì„±**: êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì •ë³´ ì œê³µ

## ì˜ë£Œ/ê±´ê°• ê´€ë ¨ ì •ë³´ ì£¼ì˜ì‚¬í•­
âš ï¸ ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ì˜ ê²½ìš°, ìƒë‹´ì„ ë³´ì¡°í•˜ëŠ” ì°¸ê³  ì •ë³´ë§Œ ì œê³µí•˜ë©° ê°œë³„ì ì¸ ì˜ë£Œ ìƒë‹´ì€ ì „ë¬¸ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ë„ë¡ ì•ˆë‚´í•©ë‹ˆë‹¤."""

    if conversation_history:
        user_message = """## ì´ì „ ëŒ€í™” ë‚´ìš©
{conversation_history}

## í˜„ì¬ ì§ˆë¬¸
{question}

ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
    else:
        user_message = """## ì§ˆë¬¸
{question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì²´ê³„ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

    prompt = ChatPromptTemplate.from_messages(
        [("system", system_message), ("user", user_message)]
    )

    invoke_params = {"question": state["question"]}

    if conversation_history:
        invoke_params["conversation_history"] = conversation_history

    resp = (prompt | llm).invoke(invoke_params)
    return {"answer": resp.content}


def n_guard_end(state: RAGState) -> RAGState:
    """ê·¸ë˜í”„ ì¢…ë£Œ ì „ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    if state.get("is_related") and not state.get("raw_docs"):
        return {"error": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}
    return {}
